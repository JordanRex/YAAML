# function to return interpretation across methods (specify which or All)

class model_interpret():
    
    def __init__():
        """ this module takes as input the model and train/test datasets to generate interpretations of the
        predictions generated by the model
        LIME and SHAP methods have been added as a provision currently, treeinterpreter will be added later
        """
    
    def lime_interpreter(feat_names, classnames, categindices, categnames, 
                         kw, num_feature, train, test, n):
        explainer = lime.lime_tabular.LimeTabularExplainer(training_data = train.values,
                                                   feature_names = list(feat_names),
                                                   class_names = classnames,
                                                   categorical_features=categindices, 
                                                   categorical_names=categnames, kernel_width = kw)
        xtest = test.values
        exp = explainer.explain_instance(xtest[n], model.predict_proba, num_features = num_feature)
        return exp.show_in_notebook()
    
    def shap_interpreter(model, train, test, n, method = 'tree'):
        """ specify n as the prediction/observation you want the interpretation to be returned for """
        
        if method == 'tree':
            # create our SHAP explainer
            shap_explainer = shap.TreeExplainer(model)
            # calculate the shapley values for our test set
            shap_values = shap_explainer.shap_values(test.values)
        elif method == 'kernel':
            # create our SHAP explainer
            shap_explainer = shap.KernelExplainer(model.predict_proba, shap.kmeans(train[:100], 5))
            shap_values = shap_explainer.shap_values(test.values)
            
        # load JS in order to use some of the plotting functions from the shap package in the notebook
        shap.initjs()
        
        # plot the explanation for a single prediction
        return shap.force_plot(shap_values[n, :], test.iloc[n, :])
    
    def model_interpreter(interpreter_algo, train, test, shap_method = 'tree', kw = 3, n = 0, model = None,
                          feat_names = None, classnames = None,
                          categindices = None, categnames = None, num_feature = None):
        if interpreter_algo == 'lime':
            return model_interpret.lime_interpreter(feat_names, classnames, categindices, categnames,
                                                    kw, num_feature, train, test, n)
        elif interpreter_algo == 'shap':
            return model_interpret.shap_interpreter(model, train, test, n, method = shap_method)
        

x = model_interpret.model_interpreter(interpreter_algo='lime', model = model, train = X_train, test = X_test, feat_names = feat_names,
                                  classnames = ['class1', 'class2'], categindices = categ_idx, categnames = categ_names,
                                 num_feature = 5, n=0)